{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import torchvision\n",
    "import pytorch3d\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "# Import toolkits\n",
    "from summer2022_toolbox.visualization_3D_objects import *\n",
    "from summer2022_toolbox.preprocessing import *\n",
    "from summer2022_toolbox.read_object import *\n",
    "from summer2022_toolbox.model_averaging import *\n",
    "from summer2022_toolbox.model_PCA import *\n",
    "from summer2022_toolbox.morphable_model import *\n",
    "from summer2022_toolbox.model_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model by dhiraj inspried from Charles\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PCAutoEncoder(nn.Module):\n",
    "    \"\"\" Autoencoder for Point Cloud \n",
    "    Input: Batch of Point Cloud B x 3 x N\n",
    "    Output: reconstructed points\n",
    "    \"\"\"\n",
    "    def __init__(self, point_dim, num_points):\n",
    "        super(PCAutoEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=point_dim, out_channels=64, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.conv5 = nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.fc3 = nn.Linear(in_features=1024, out_features=num_points*3)\n",
    "\n",
    "        #batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        point_dim = x.shape[1]\n",
    "        num_points = x.shape[2]\n",
    "\n",
    "        #encoder\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1(self.conv2(x)))\n",
    "        x = F.relu(self.bn1(self.conv3(x)))\n",
    "        x = F.relu(self.bn2(self.conv4(x)))\n",
    "        x = F.relu(self.bn3(self.conv5(x)))\n",
    "\n",
    "        # do max pooling \n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        # get the global embedding\n",
    "        global_feat = x\n",
    "\n",
    "        #decoder\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = F.relu(self.bn3(self.fc2(x)))\n",
    "        reconstructed_points = self.fc3(x)\n",
    "\n",
    "        #do reshaping\n",
    "        reconstructed_points = reconstructed_points.reshape(batch_size, point_dim, num_points)\n",
    "\n",
    "        return reconstructed_points , global_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU...\n",
      "Forward pass successful\n",
      "conv1      (64, 3, 1)           192       \n",
      "conv2      (64, 64, 1)          4096      \n",
      "conv4      (128, 64, 1)         8192      \n",
      "conv5      (1024, 128, 1)       131072    \n",
      "fc1        (1024, 1024)         1048576   \n",
      "fc3        (6144, 1024)         6291456   \n"
     ]
    }
   ],
   "source": [
    "# GPU check                \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device =='cuda':\n",
    "    print(\"Run on GPU...\")\n",
    "else:\n",
    "    print(\"Run on CPU...\")\n",
    "\n",
    "# Model Definition  \n",
    "net = PCAutoEncoder(3, 2048)\n",
    "net = net.to(device)\n",
    "\n",
    "# Test forward pass\n",
    "data = torch.randn(5,3,2048).to(device)\n",
    "# Forward pass \"data\" through \"net\" to get output \"out\" \n",
    "out = net.forward(data)   #Your code here\n",
    "\n",
    "# Check output shape\n",
    "assert(out[0].detach().cpu().numpy().shape == (5,3,2048))\n",
    "print(\"Forward pass successful\")\n",
    "\n",
    "# Shape Observation\n",
    "# Forward pass of a single image\n",
    "data = torch.randn(5,3,2048).to(device)\n",
    "# Forward pass \"data\" through \"net\" to get output \"out\" \n",
    "out = net.forward(data)   #Your code here\n",
    "\n",
    "# Print the output shape and number of parameteres throughout the network\n",
    "for name, module in net.named_modules():\n",
    "    if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "        # # Get the weight of the module as a NumPy array\n",
    "        weight = module.weight.cpu().detach().numpy()     #Your code here\n",
    "        # # Compute the number of parameters in the weight\n",
    "        num_Param = np.prod(weight.shape)  #Your code here\n",
    "        \n",
    "        print(f'{name:10} {str(weight.shape):20} {str(num_Param):10}')\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Preprocessed ModelNet40 cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4929)\n"
     ]
    }
   ],
   "source": [
    "#load all aligned cars\n",
    "f1 = open('data/preprocessed/car/train_aligned.txt','rb')\n",
    "X_train = pickle.load(f1)\n",
    "f2 = open('data/preprocessed/car/test_aligned.txt','rb')\n",
    "X_test = pickle.load(f2)\n",
    "\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 12\n",
    "VAL_BATCH_SIZE = 10\n",
    "\n",
    "# construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    X_train, \n",
    "    batch_size=TRAIN_BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    X_test, \n",
    "    batch_size=VAL_BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU...\n"
     ]
    }
   ],
   "source": [
    "# GPU check                \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device =='cuda':\n",
    "    print(\"Run on GPU...\")\n",
    "else:\n",
    "    print(\"Run on CPU...\")\n",
    "\n",
    "# Model Definition  \n",
    "model = PCAutoEncoder(3, 4929)\n",
    "model = model.to(device)\n",
    "\n",
    "# Check if on GPU\n",
    "assert(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance\n",
    "# initial learning rate = 0.1, decay rate = 0.1\n",
    "INITIAL_LR = 0.001\n",
    "\n",
    "# momentum for optimizer = 0.9\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# L2 regularization strength = 0.0001\n",
    "REG = 1e-4\n",
    "\n",
    "# create loss function: Chamfer distance\n",
    "criterion = lambda recon_x, x: chamfer_distance(recon_x, x).to(device)\n",
    "\n",
    "# Add optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=INITIAL_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 3.94 GiB total capacity; 2.22 GiB already allocated; 181.12 MiB free; 2.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# compute the output and loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m dist1, dist2 \u001b[39m=\u001b[39m criterion(outputs, inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mmean(dist1)) \u001b[39m+\u001b[39m (torch\u001b[39m.\u001b[39mmean(dist2))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb Cell 12\u001b[0m in \u001b[0;36mPCAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# x = F.relu(self.bn1(self.conv3(x)))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn3(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv5(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# do max pooling \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.237.17.229/home/warrenzhao/yuzhou/VAE_3D/VAE_3D.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(x, \u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/functional.py:1298\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1298\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1299\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 3.94 GiB total capacity; 2.22 GiB already allocated; 181.12 MiB free; 2.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# some hyperparameters\n",
    "# total number of training epochs\n",
    "EPOCHS = 2\n",
    "DECAY_EPOCHS = 70\n",
    "DECAY = 0.7\n",
    "# DECAY = 0.1\n",
    "\n",
    "# the folder where the trained model is saved\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "\n",
    "# start the training/validation process\n",
    "# the process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "best_loss = 1e20\n",
    "current_learning_rate = INITIAL_LR\n",
    "\n",
    "start = time.time()\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# store loss learning curve\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "\n",
    "for i in range(0, EPOCHS):\n",
    "    # handle the learning rate scheduler.\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate * DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    train_loss = 0 # track training loss if you want\n",
    "    \n",
    "    # Train the model for 1 epoch.\n",
    "    for batch_idx, (inputs) in enumerate(train_loader):\n",
    "        # copy inputs to device\n",
    "        inputs = inputs.float().to(device)\n",
    "\n",
    "        # compute the output and loss\n",
    "        outputs = model(inputs)\n",
    "        dist1, dist2 = criterion(outputs, inputs)\n",
    "\n",
    "        loss = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "        train_loss += loss\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # apply gradient and update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(\"Training loss: %.4f\" %(avg_loss))\n",
    "    train_loss_lst.append(avg_loss.to('cpu').detach().numpy())\n",
    "\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # this help you compute the validation accuracy\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    val_loss = 0 # again, track the validation loss if you want\n",
    "    \n",
    "    # disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs) in enumerate(val_loader):\n",
    "            # copy inputs to device\n",
    "            inputs = inputs.float().to(device)\n",
    "            # compute the output and loss\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            outputs = outputs.transpose(1, 2)\n",
    "\n",
    "            dist1, dist2 = criterion(outputs, inputs)\n",
    "            print(dist1, dist2)\n",
    "\n",
    "            loss = (torch.mean(dist1)) + (torch.mean(dist2))\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    print(\"Validation loss: %.4f\" % (avg_loss))\n",
    "    \n",
    "    valid_loss_lst.append(avg_loss.cpu().detach().numpy())\n",
    "    \n",
    "    # save the model checkpoint\n",
    "    if avg_loss < best_loss:\n",
    "        best_val_acc = avg_loss\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "            os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': model.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'resnet20.pth'))\n",
    "        \n",
    "    print('')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"==> Optimization finished in {time.time() - start:.2f}s! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch3d': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7f2a7996328504c9da638a538617e092c5f6c113e142544f6d693b60dd4d365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
